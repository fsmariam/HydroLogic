{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water Pump Project: Baseline Modeling Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå Section 1: Overview & Purpose\n",
    "\"\"\"\n",
    "This notebook establishes baseline models for the Tanzania Water Pump classification project.\n",
    "Purpose:\n",
    "- Provide a reference point before feature engineering\n",
    "- Use consistent evaluation to compare future improvements\n",
    "- Enable collaborative work where each team member can track their model impact\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì¶ Section 2: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÇ Section 3: Load Data (Change this path for your version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/processed/cleaned_merged_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "df = df.drop(columns=[\"latitude_was_outlier\", \"longitude_was_outlier\", \"extraction_mismatch\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Section 4: Define Features & Target (adjust as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"status_group\", axis=1)  # Replace with your actual target column\n",
    "y = data[\"status_group\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÑ Fit vs Transform Explanation\n",
    "\"\"\"\n",
    "What is the difference between `fit()` and `transform()`?\n",
    "- `fit()`: Learns the parameters from data (e.g., mean, std, most frequent value).\n",
    "- `transform()`: Applies those learned parameters to transform the data.\n",
    "- `fit_transform()`: A convenience method that combines both for training data.\n",
    "\n",
    "Use `fit()` only on the training set. Then `transform()` both training and validation/test using the learned parameters.\n",
    "This avoids data leakage.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü•™ Section 4.1: Fit vs Transform Code Examples\n",
    "\"\"\"\n",
    "Example: StandardScaler on training and validation sets\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit only on training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform both sets using the same scaler\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example: OneHotEncoder and SimpleImputer with training/validation data\n",
    "\"\"\"\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define columns\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Define transformers\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine into a column transformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Fit on training, transform both\n",
    "preprocessor.fit(X_train)\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåê Section 4.2: Preprocessing Function\n",
    "\"\"\"\n",
    "Reusable preprocessing function for training and validation data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X_train, X_val):\n",
    "    numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "    preprocessor.fit(X_train)\n",
    "    X_train_processed = preprocessor.transform(X_train)\n",
    "    X_val_processed = preprocessor.transform(X_val)\n",
    "\n",
    "    return X_train_processed, X_val_processed, preprocessor\n",
    "\n",
    "# Example usage:\n",
    "# X_train_proc, X_val_proc, processor = preprocess_data(X_train, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to preprocess new (e.g. test) data using fitted preprocessor\n",
    "\"\"\"\n",
    "def preprocess_new_data(X_new, fitted_preprocessor):\n",
    "    return fitted_preprocessor.transform(X_new)\n",
    "\n",
    "# ü•™ Section 5: Stratified Train/Validation Split\n",
    "# Stratification ensures the class proportions are maintained in both training and validation sets.\n",
    "# This is important for fair baseline evaluation before applying class imbalance techniques (like SMOTE).\n",
    "# Do not apply oversampling to the validation set ‚Äî this should reflect the original distribution.\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Class distribution in training:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nClass distribution in validation:\")\n",
    "print(y_val.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Section 5: Stratified Train/Validation Split\n",
    "\n",
    "- Stratification ensures the class proportions are maintained in both training and validation sets.\n",
    "- This is important for fair baseline evaluation before applying class imbalance techniques (like SMOTE).\n",
    "- Do not apply oversampling to the validation set ‚Äî this should reflect the original distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in training:\n",
      "status_group\n",
      "functional                 0.545089\n",
      "non functional             0.386636\n",
      "functional needs repair    0.068275\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class distribution in validation:\n",
      "status_group\n",
      "functional                 0.545107\n",
      "non functional             0.386646\n",
      "functional needs repair    0.068247\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Class distribution in training:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nClass distribution in validation:\")\n",
    "print(y_val.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Section 6: Dummy Classifier (Naive Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dummy Classifier Report:\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "             functional       0.55      1.00      0.71      6278\n",
      "functional needs repair       0.00      0.00      0.00       786\n",
      "         non functional       0.00      0.00      0.00      4453\n",
      "\n",
      "               accuracy                           0.55     11517\n",
      "              macro avg       0.18      0.33      0.24     11517\n",
      "           weighted avg       0.30      0.55      0.38     11517\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariam/Documents/DC/HydroLogic/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/mariam/Documents/DC/HydroLogic/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/mariam/Documents/DC/HydroLogic/.venv/lib/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train, y_train)\n",
    "y_dummy = dummy.predict(X_val)\n",
    "print(\"\\nDummy Classifier Report:\")\n",
    "print(classification_report(y_val, y_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üå≥ Section 7: Basic Models (Logistic & Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Germany Republi'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m results = []\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models.items():\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     y_pred = model.predict(X_val)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Report:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DC/HydroLogic/.venv/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DC/HydroLogic/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1222\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1219\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1220\u001b[39m     _dtype = [np.float64, np.float32]\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mliblinear\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msag\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msaga\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1231\u001b[39m check_classification_targets(y)\n\u001b[32m   1232\u001b[39m \u001b[38;5;28mself\u001b[39m.classes_ = np.unique(y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DC/HydroLogic/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2961\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2959\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2960\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2961\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2962\u001b[39m     out = X, y\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DC/HydroLogic/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1370\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1364\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1365\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1366\u001b[39m     )\n\u001b[32m   1368\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1389\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DC/HydroLogic/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:973\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pandas_requires_conversion:\n\u001b[32m    969\u001b[39m     \u001b[38;5;66;03m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[32m    970\u001b[39m     \u001b[38;5;66;03m# nans\u001b[39;00m\n\u001b[32m    971\u001b[39m     \u001b[38;5;66;03m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[32m    972\u001b[39m     new_dtype = dtype_orig \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m     array = \u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m     \u001b[38;5;66;03m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[32m    975\u001b[39m     dtype = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DC/HydroLogic/.venv/lib/python3.13/site-packages/pandas/core/generic.py:6643\u001b[39m, in \u001b[36mNDFrame.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m   6637\u001b[39m     results = [\n\u001b[32m   6638\u001b[39m         ser.astype(dtype, copy=copy, errors=errors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items()\n\u001b[32m   6639\u001b[39m     ]\n\u001b[32m   6641\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6642\u001b[39m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6643\u001b[39m     new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6644\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes)\n\u001b[32m   6645\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mastype\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DC/HydroLogic/.venv/lib/python3.13/site-packages/pandas/core/internals/managers.py:430\u001b[39m, in \u001b[36mBaseBlockManager.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[32m    428\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mastype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DC/HydroLogic/.venv/lib/python3.13/site-packages/pandas/core/internals/managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DC/HydroLogic/.venv/lib/python3.13/site-packages/pandas/core/internals/blocks.py:758\u001b[39m, in \u001b[36mBlock.astype\u001b[39m\u001b[34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[39m\n\u001b[32m    755\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan not squeeze with more than one column.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    756\u001b[39m     values = values[\u001b[32m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m new_values = \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m new_values = maybe_coerce_values(new_values)\n\u001b[32m    762\u001b[39m refs = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DC/HydroLogic/.venv/lib/python3.13/site-packages/pandas/core/dtypes/astype.py:237\u001b[39m, in \u001b[36mastype_array_safe\u001b[39m\u001b[34m(values, dtype, copy, errors)\u001b[39m\n\u001b[32m    234\u001b[39m     dtype = dtype.numpy_dtype\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     new_values = \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DC/HydroLogic/.venv/lib/python3.13/site-packages/pandas/core/dtypes/astype.py:182\u001b[39m, in \u001b[36mastype_array\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    179\u001b[39m     values = values.astype(dtype, copy=copy)\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     values = \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np.dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/DC/HydroLogic/.venv/lib/python3.13/site-packages/pandas/core/dtypes/astype.py:133\u001b[39m, in \u001b[36m_astype_nansafe\u001b[39m\u001b[34m(arr, dtype, copy, skipna)\u001b[39m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype == \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m arr.astype(dtype, copy=copy)\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'Germany Republi'"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    print(f\"\\n{name} Report:\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": model.score(X_val, y_val),\n",
    "        \"F1 Macro\": cross_val_score(model, X, y, cv=5, scoring=\"f1_macro\").mean()\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßÆ Section 8: Evaluation Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nBenchmark Comparison Table:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Section 9: Confusion Matrix Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ConfusionMatrixDisplay.from_estimator(models[\"Decision Tree\"], X_val, y_val, ax=ax)\n",
    "plt.title(\"Decision Tree Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß™ Section 10: Feature Testing Cell\n",
    "\"\"\"\n",
    "Use this cell to test modified datasets:\n",
    "- Replace X and y with your processed versions\n",
    "- Re-run training and evaluation to see improvement\n",
    "\"\"\"\n",
    "# Example:\n",
    "# X_new = processed_data.drop(\"status_group\", axis=1)\n",
    "# y_new = processed_data[\"status_group\"]\n",
    "# Then re-run model training and update benchmark table\n",
    "\n",
    "# üìù Notes:\n",
    "# - Make sure all team members use the same seed (random_state=42)\n",
    "# - Save your results to outputs/ folder if needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
