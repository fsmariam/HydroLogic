{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e029c9e",
   "metadata": {},
   "source": [
    "It does not make sence that i clean the dataset and then test each approach because i wanna check to see which approch is better for the imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e238e6",
   "metadata": {},
   "source": [
    "# Cleaning the Data and test the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3ef04ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>amount_tsh</th>\n",
       "      <th>gps_height</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>num_private</th>\n",
       "      <th>region_code</th>\n",
       "      <th>district_code</th>\n",
       "      <th>population</th>\n",
       "      <th>construction_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>5.940000e+04</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>37115.131768</td>\n",
       "      <td>317.650385</td>\n",
       "      <td>668.297239</td>\n",
       "      <td>34.077427</td>\n",
       "      <td>-5.706033e+00</td>\n",
       "      <td>0.474141</td>\n",
       "      <td>15.297003</td>\n",
       "      <td>5.629747</td>\n",
       "      <td>179.909983</td>\n",
       "      <td>1300.652475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>21453.128371</td>\n",
       "      <td>2997.574558</td>\n",
       "      <td>693.116350</td>\n",
       "      <td>6.567432</td>\n",
       "      <td>2.946019e+00</td>\n",
       "      <td>12.236230</td>\n",
       "      <td>17.587406</td>\n",
       "      <td>9.633649</td>\n",
       "      <td>471.482176</td>\n",
       "      <td>951.620547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-90.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.164944e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18519.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.090347</td>\n",
       "      <td>-8.540621e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37061.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>369.000000</td>\n",
       "      <td>34.908743</td>\n",
       "      <td>-5.021597e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1986.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>55656.500000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1319.250000</td>\n",
       "      <td>37.178387</td>\n",
       "      <td>-3.326156e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>74247.000000</td>\n",
       "      <td>350000.000000</td>\n",
       "      <td>2770.000000</td>\n",
       "      <td>40.345193</td>\n",
       "      <td>-2.000000e-08</td>\n",
       "      <td>1776.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>30500.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     amount_tsh    gps_height     longitude      latitude  \\\n",
       "count  59400.000000   59400.000000  59400.000000  59400.000000  5.940000e+04   \n",
       "mean   37115.131768     317.650385    668.297239     34.077427 -5.706033e+00   \n",
       "std    21453.128371    2997.574558    693.116350      6.567432  2.946019e+00   \n",
       "min        0.000000       0.000000    -90.000000      0.000000 -1.164944e+01   \n",
       "25%    18519.750000       0.000000      0.000000     33.090347 -8.540621e+00   \n",
       "50%    37061.500000       0.000000    369.000000     34.908743 -5.021597e+00   \n",
       "75%    55656.500000      20.000000   1319.250000     37.178387 -3.326156e+00   \n",
       "max    74247.000000  350000.000000   2770.000000     40.345193 -2.000000e-08   \n",
       "\n",
       "        num_private   region_code  district_code    population  \\\n",
       "count  59400.000000  59400.000000   59400.000000  59400.000000   \n",
       "mean       0.474141     15.297003       5.629747    179.909983   \n",
       "std       12.236230     17.587406       9.633649    471.482176   \n",
       "min        0.000000      1.000000       0.000000      0.000000   \n",
       "25%        0.000000      5.000000       2.000000      0.000000   \n",
       "50%        0.000000     12.000000       3.000000     25.000000   \n",
       "75%        0.000000     17.000000       5.000000    215.000000   \n",
       "max     1776.000000     99.000000      80.000000  30500.000000   \n",
       "\n",
       "       construction_year  \n",
       "count       59400.000000  \n",
       "mean         1300.652475  \n",
       "std           951.620547  \n",
       "min             0.000000  \n",
       "25%             0.000000  \n",
       "50%          1986.000000  \n",
       "75%          2004.000000  \n",
       "max          2013.000000  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# save filepath to variable for easier access\n",
    "pump_file_path = '../data/processed/pump_dataset.csv'\n",
    "# read the data and store data in DataFrame titled pomp_data\n",
    "pump_data = pd.read_csv(pump_file_path) \n",
    "# print a summary of the data in water pomp data\n",
    "pump_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c52aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.experimental import enable_iterative_imputer  # This enables IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "# Transformer: Replace 0 with random longitude\n",
    "class LongitudeRandomImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column='longitude', min_val=29.2, max_val=40.4):\n",
    "        self.column = column\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[self.column] = X[self.column].replace(0, np.nan)\n",
    "        X[self.column] = X[self.column].apply(\n",
    "            lambda x: np.random.uniform(self.min_val, self.max_val) if pd.isna(x) else x\n",
    "        )\n",
    "        return X\n",
    "\n",
    "# Transformer: Group-based mode imputation with global fallback\n",
    "class GroupModeImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, col, group_cols):\n",
    "        self.col = col\n",
    "        self.group_cols = group_cols\n",
    "        self.global_mode = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.global_mode = X[self.col].mode(dropna=True)[0]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        try:\n",
    "            X[self.col] = X.groupby(self.group_cols)[self.col].transform(\n",
    "                lambda x: x.fillna(x.mode()[0]) if not x.mode().empty else x\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Warning during group imputation of {self.col}: {e}\")\n",
    "        X[self.col].fillna(self.global_mode, inplace=True)\n",
    "        return X\n",
    "\n",
    "# Transformer: Iterative imputer for numeric columns\n",
    "class IterativeNumericImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.imputer = IterativeImputer(estimator=BayesianRidge(), random_state=42, max_iter=10)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.imputer.fit(X[self.columns])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[self.columns] = self.imputer.transform(X[self.columns])\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc84ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# List of all imputers and their logic\n",
    "imputation_pipeline = Pipeline(steps=[\n",
    "    ('longitude_random', LongitudeRandomImputer()),\n",
    "\n",
    "    ('funder_impute', GroupModeImputer('funder', ['region', 'lga', 'ward'])),\n",
    "    ('installer_impute', GroupModeImputer('installer', ['region', 'funder'])),\n",
    "    ('subvillage_impute', GroupModeImputer('subvillage', ['lga', 'ward'])),\n",
    "    ('public_meeting_impute', GroupModeImputer('public_meeting', ['region', 'ward'])),\n",
    "    ('scheme_name_impute', GroupModeImputer('scheme_name', ['basin', 'region', 'longitude', 'latitude'])),\n",
    "    ('scheme_management_impute', GroupModeImputer('scheme_management', ['scheme_name', 'source', 'region'])),\n",
    "    ('permit_impute', GroupModeImputer('permit', ['region', 'source'])),\n",
    "\n",
    "    ('iterative_numeric', IterativeNumericImputer(['construction_year', 'population', 'latitude', 'longitude', 'gps_height']))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "761cf2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target from predictors\n",
    "y = pump_data.status_group\n",
    "X = pump_data.drop(['status_group'], axis=1)\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "X_train_full = X_train_full.copy()\n",
    "X_valid_full = X_valid_full.copy() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed8614b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the training data\n",
    "X_train_full = imputation_pipeline.fit_transform(X_train_full)\n",
    "\n",
    "# Only transform the validation set\n",
    "X_valid_full = imputation_pipeline.transform(X_valid_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "343f000d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Drop columns\n",
    "drop_columns = ['id','recorded_by', 'num_private', 'payment']\n",
    "X_train_full.drop(drop_columns, axis=1, inplace=True)\n",
    "X_valid_full.drop(drop_columns, axis=1, inplace=True)\n",
    "\n",
    "# Drop columns with missing values (simplest approach)\n",
    "cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()] \n",
    "print(cols_with_missing)\n",
    "X_train_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_valid_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8f49bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "amount_tsh               0\n",
       "date_recorded            0\n",
       "construction_year        0\n",
       "extraction_type          0\n",
       "extraction_type_group    0\n",
       "extraction_type_class    0\n",
       "management               0\n",
       "management_group         0\n",
       "payment_type             0\n",
       "water_quality            0\n",
       "quality_group            0\n",
       "quantity                 0\n",
       "quantity_group           0\n",
       "source                   0\n",
       "source_type              0\n",
       "source_class             0\n",
       "waterpoint_type          0\n",
       "permit                   0\n",
       "scheme_name              0\n",
       "scheme_management        0\n",
       "basin                    0\n",
       "funder                   0\n",
       "gps_height               0\n",
       "installer                0\n",
       "longitude                0\n",
       "latitude                 0\n",
       "wpt_name                 0\n",
       "subvillage               0\n",
       "public_meeting           0\n",
       "region                   0\n",
       "region_code              0\n",
       "district_code            0\n",
       "lga                      0\n",
       "ward                     0\n",
       "population               0\n",
       "waterpoint_type_group    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_counts = X_train_full.isnull().sum().sort_values(ascending=False)\n",
    "missing_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8372c503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables:\n",
      "['basin', 'extraction_type_class', 'management_group', 'payment_type', 'water_quality', 'quality_group', 'quantity', 'quantity_group', 'source_type', 'source_class', 'waterpoint_type', 'waterpoint_type_group']\n"
     ]
    }
   ],
   "source": [
    "# Get list of categorical variables\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe66ca22",
   "metadata": {},
   "source": [
    "## Define Quality for each approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32fc3eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8844576d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from Approach 1 (Drop categorical variables):\n",
      "0.7209595959595959\n"
     ]
    }
   ],
   "source": [
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "print(\"Accuracy from Approach 1 (Drop categorical variables):\")\n",
    "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e3e8e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 2 (Ordinal Encoding):\n",
      "0.8051346801346801\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Make copy to avoid changing original data \n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Apply ordinal encoder to each column with categorical data\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
    "label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\n",
    "\n",
    "print(\"MAE from Approach 2 (Ordinal Encoding):\") \n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ce63f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 3 (One-Hot Encoding):\n",
      "0.8002525252525252\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "# Ensure all columns have string type\n",
    "OH_X_train.columns = OH_X_train.columns.astype(str)\n",
    "OH_X_valid.columns = OH_X_valid.columns.astype(str)\n",
    "\n",
    "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
