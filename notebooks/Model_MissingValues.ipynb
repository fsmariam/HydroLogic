{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "197dfbe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount_tsh</th>\n",
       "      <th>gps_height</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>region_code</th>\n",
       "      <th>district_code</th>\n",
       "      <th>population</th>\n",
       "      <th>construction_year</th>\n",
       "      <th>year_recorded</th>\n",
       "      <th>month_recorded</th>\n",
       "      <th>dayofweek_recorded</th>\n",
       "      <th>pump_age</th>\n",
       "      <th>location_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>5.940000e+04</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "      <td>59400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>317.650385</td>\n",
       "      <td>668.297239</td>\n",
       "      <td>35.136692</td>\n",
       "      <td>-5.706033e+00</td>\n",
       "      <td>15.297003</td>\n",
       "      <td>5.629747</td>\n",
       "      <td>179.909983</td>\n",
       "      <td>1996.254495</td>\n",
       "      <td>2011.921667</td>\n",
       "      <td>4.375640</td>\n",
       "      <td>2.939933</td>\n",
       "      <td>15.667172</td>\n",
       "      <td>3.761549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2997.574558</td>\n",
       "      <td>693.116350</td>\n",
       "      <td>2.628459</td>\n",
       "      <td>2.946019e+00</td>\n",
       "      <td>17.587406</td>\n",
       "      <td>9.633649</td>\n",
       "      <td>471.482176</td>\n",
       "      <td>10.143783</td>\n",
       "      <td>0.958758</td>\n",
       "      <td>3.029247</td>\n",
       "      <td>1.951627</td>\n",
       "      <td>10.157652</td>\n",
       "      <td>2.820036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-90.000000</td>\n",
       "      <td>29.219817</td>\n",
       "      <td>-1.164944e+01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1960.000000</td>\n",
       "      <td>2002.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.249501</td>\n",
       "      <td>-8.540621e+00</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1993.000000</td>\n",
       "      <td>2011.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>369.000000</td>\n",
       "      <td>35.000424</td>\n",
       "      <td>-5.021597e+00</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1996.000000</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>1319.250000</td>\n",
       "      <td>37.241238</td>\n",
       "      <td>-3.326156e+00</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>350000.000000</td>\n",
       "      <td>2770.000000</td>\n",
       "      <td>40.391390</td>\n",
       "      <td>-2.000000e-08</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>30500.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          amount_tsh    gps_height     longitude      latitude   region_code  \\\n",
       "count   59400.000000  59400.000000  59400.000000  5.940000e+04  59400.000000   \n",
       "mean      317.650385    668.297239     35.136692 -5.706033e+00     15.297003   \n",
       "std      2997.574558    693.116350      2.628459  2.946019e+00     17.587406   \n",
       "min         0.000000    -90.000000     29.219817 -1.164944e+01      1.000000   \n",
       "25%         0.000000      0.000000     33.249501 -8.540621e+00      5.000000   \n",
       "50%         0.000000    369.000000     35.000424 -5.021597e+00     12.000000   \n",
       "75%        20.000000   1319.250000     37.241238 -3.326156e+00     17.000000   \n",
       "max    350000.000000   2770.000000     40.391390 -2.000000e-08     99.000000   \n",
       "\n",
       "       district_code    population  construction_year  year_recorded  \\\n",
       "count   59400.000000  59400.000000       59400.000000   59400.000000   \n",
       "mean        5.629747    179.909983        1996.254495    2011.921667   \n",
       "std         9.633649    471.482176          10.143783       0.958758   \n",
       "min         0.000000      0.000000        1960.000000    2002.000000   \n",
       "25%         2.000000      0.000000        1993.000000    2011.000000   \n",
       "50%         3.000000     25.000000        1996.000000    2012.000000   \n",
       "75%         5.000000    215.000000        2004.000000    2013.000000   \n",
       "max        80.000000  30500.000000        2013.000000    2013.000000   \n",
       "\n",
       "       month_recorded  dayofweek_recorded      pump_age  location_cluster  \n",
       "count    59400.000000        59400.000000  59400.000000      59400.000000  \n",
       "mean         4.375640            2.939933     15.667172          3.761549  \n",
       "std          3.029247            1.951627     10.157652          2.820036  \n",
       "min          1.000000            0.000000     -7.000000          0.000000  \n",
       "25%          2.000000            1.000000      8.000000          1.000000  \n",
       "50%          3.000000            3.000000     15.000000          3.000000  \n",
       "75%          7.000000            5.000000     19.000000          6.000000  \n",
       "max         12.000000            6.000000     53.000000          9.000000  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# save filepath to variable for easier access\n",
    "#pump_file_path = '../data/processed/pump_dataset.csv'\n",
    "# this is a merged full file without missed values\n",
    "pump_file_path = \"../data/processed/pump_dataset_full_cleaned.csv\"\n",
    "\n",
    "# read the data and store data in DataFrame titled pomp_data\n",
    "pump_data = pd.read_csv(pump_file_path) \n",
    "# print a summary of the data in water pomp data\n",
    "pump_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d53f1d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['amount_tsh', 'funder', 'gps_height', 'installer', 'longitude',\n",
       "       'latitude', 'wpt_name', 'basin', 'subvillage', 'region', 'region_code',\n",
       "       'district_code', 'lga', 'ward', 'population', 'public_meeting',\n",
       "       'scheme_management', 'scheme_name', 'permit', 'construction_year',\n",
       "       'extraction_type', 'extraction_type_group', 'extraction_type_class',\n",
       "       'management', 'management_group', 'payment_type', 'water_quality',\n",
       "       'quality_group', 'quantity', 'quantity_group', 'source', 'source_type',\n",
       "       'source_class', 'waterpoint_type', 'waterpoint_type_group',\n",
       "       'status_group', 'year_recorded', 'month_recorded', 'dayofweek_recorded',\n",
       "       'funder_installer', 'pump_age', 'location_cluster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the list of all columns in the dataset to choose the varieables/co.umns\n",
    "pump_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7fa3ff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target from predictors\n",
    "y = pump_data.status_group\n",
    "X = pump_data.drop(['status_group'], axis=1)\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "X_train_full = X_train_full.copy()\n",
    "X_valid_full = X_valid_full.copy() \n",
    "\n",
    "# Drop columns with missing values (simplest approach)\n",
    "#cols_with_missing = ['id','recorded_by', 'num_private', 'payment']\n",
    "cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()] \n",
    "X_train_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_valid_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "24620cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basin</th>\n",
       "      <th>extraction_type_class</th>\n",
       "      <th>management_group</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>water_quality</th>\n",
       "      <th>quality_group</th>\n",
       "      <th>quantity</th>\n",
       "      <th>quantity_group</th>\n",
       "      <th>source_type</th>\n",
       "      <th>source_class</th>\n",
       "      <th>...</th>\n",
       "      <th>latitude</th>\n",
       "      <th>region_code</th>\n",
       "      <th>district_code</th>\n",
       "      <th>population</th>\n",
       "      <th>construction_year</th>\n",
       "      <th>year_recorded</th>\n",
       "      <th>month_recorded</th>\n",
       "      <th>dayofweek_recorded</th>\n",
       "      <th>pump_age</th>\n",
       "      <th>location_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55424</th>\n",
       "      <td>Pangani</td>\n",
       "      <td>gravity</td>\n",
       "      <td>user-group</td>\n",
       "      <td>never pay</td>\n",
       "      <td>soft</td>\n",
       "      <td>good</td>\n",
       "      <td>enough</td>\n",
       "      <td>enough</td>\n",
       "      <td>spring</td>\n",
       "      <td>groundwater</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.783454</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1994</td>\n",
       "      <td>2011</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28170</th>\n",
       "      <td>Internal</td>\n",
       "      <td>submersible</td>\n",
       "      <td>user-group</td>\n",
       "      <td>per bucket</td>\n",
       "      <td>salty</td>\n",
       "      <td>salty</td>\n",
       "      <td>insufficient</td>\n",
       "      <td>insufficient</td>\n",
       "      <td>borehole</td>\n",
       "      <td>groundwater</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.309293</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1997</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20007</th>\n",
       "      <td>Internal</td>\n",
       "      <td>gravity</td>\n",
       "      <td>user-group</td>\n",
       "      <td>never pay</td>\n",
       "      <td>soft</td>\n",
       "      <td>good</td>\n",
       "      <td>insufficient</td>\n",
       "      <td>insufficient</td>\n",
       "      <td>spring</td>\n",
       "      <td>groundwater</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.217454</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1974</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7842</th>\n",
       "      <td>Pangani</td>\n",
       "      <td>gravity</td>\n",
       "      <td>commercial</td>\n",
       "      <td>never pay</td>\n",
       "      <td>soft</td>\n",
       "      <td>good</td>\n",
       "      <td>enough</td>\n",
       "      <td>enough</td>\n",
       "      <td>spring</td>\n",
       "      <td>groundwater</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.141919</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1988</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22217</th>\n",
       "      <td>Lake Tanganyika</td>\n",
       "      <td>gravity</td>\n",
       "      <td>user-group</td>\n",
       "      <td>monthly</td>\n",
       "      <td>soft</td>\n",
       "      <td>good</td>\n",
       "      <td>enough</td>\n",
       "      <td>enough</td>\n",
       "      <td>river/lake</td>\n",
       "      <td>surface</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.534989</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>320.0</td>\n",
       "      <td>1999</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 basin extraction_type_class management_group payment_type  \\\n",
       "55424          Pangani               gravity       user-group    never pay   \n",
       "28170         Internal           submersible       user-group   per bucket   \n",
       "20007         Internal               gravity       user-group    never pay   \n",
       "7842           Pangani               gravity       commercial    never pay   \n",
       "22217  Lake Tanganyika               gravity       user-group      monthly   \n",
       "\n",
       "      water_quality quality_group      quantity quantity_group source_type  \\\n",
       "55424          soft          good        enough         enough      spring   \n",
       "28170         salty         salty  insufficient   insufficient    borehole   \n",
       "20007          soft          good  insufficient   insufficient      spring   \n",
       "7842           soft          good        enough         enough      spring   \n",
       "22217          soft          good        enough         enough  river/lake   \n",
       "\n",
       "      source_class  ...  latitude region_code  district_code  population  \\\n",
       "55424  groundwater  ... -4.783454           4              1         1.0   \n",
       "28170  groundwater  ... -5.309293          21              4         1.0   \n",
       "20007  groundwater  ... -3.217454           2              5       180.0   \n",
       "7842   groundwater  ... -3.141919           3              1         1.0   \n",
       "22217      surface  ... -4.534989          16              2       320.0   \n",
       "\n",
       "       construction_year  year_recorded  month_recorded  dayofweek_recorded  \\\n",
       "55424               1994           2011               4                   3   \n",
       "28170               1997           2013               2                   1   \n",
       "20007               1974           2013               3                   5   \n",
       "7842                1988           2013               2                   0   \n",
       "22217               1999           2013               2                   5   \n",
       "\n",
       "       pump_age  location_cluster  \n",
       "55424        17                 0  \n",
       "28170        16                 3  \n",
       "20007        39                 3  \n",
       "7842         25                 5  \n",
       "22217        14                 9  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e221a9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical variables:\n",
      "['basin', 'extraction_type_class', 'management_group', 'payment_type', 'water_quality', 'quality_group', 'quantity', 'quantity_group', 'source_type', 'source_class', 'waterpoint_type', 'waterpoint_type_group']\n"
     ]
    }
   ],
   "source": [
    "# Get list of categorical variables\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e0ca6",
   "metadata": {},
   "source": [
    "## Define the quality of each Approch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4ba96cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3f51fa29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from Approach 1 (Drop categorical variables):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7233164983164984\n"
     ]
    }
   ],
   "source": [
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "print(\"Accuracy from Approach 1 (Drop categorical variables):\")\n",
    "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0f5b6798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 2 (Ordinal Encoding):\n",
      "0.8067340067340067\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Make copy to avoid changing original data \n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Apply ordinal encoder to each column with categorical data\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
    "label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\n",
    "\n",
    "print(\"MAE from Approach 2 (Ordinal Encoding):\") \n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a51ba77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 3 (One-Hot Encoding):\n",
      "0.8048821548821549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "# Ensure all columns have string type\n",
    "OH_X_train.columns = OH_X_train.columns.astype(str)\n",
    "OH_X_valid.columns = OH_X_valid.columns.astype(str)\n",
    "\n",
    "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e238e6",
   "metadata": {},
   "source": [
    "# Cleaning the Data and test the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f3ef04ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport pandas as pd\\n# save filepath to variable for easier access\\npump_file_path = '../data/processed/pump_dataset.csv'\\n# read the data and store data in DataFrame titled pomp_data\\npump_data_clean = pd.read_csv(pump_file_path) \\n# print a summary of the data in water pomp data\\npump_data_clean.describe()\\n\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import pandas as pd\n",
    "# save filepath to variable for easier access\n",
    "pump_file_path = '../data/processed/pump_dataset.csv'\n",
    "# read the data and store data in DataFrame titled pomp_data\n",
    "pump_data_clean = pd.read_csv(pump_file_path) \n",
    "# print a summary of the data in water pomp data\n",
    "pump_data_clean.describe()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3c52aaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.base import BaseEstimator, TransformerMixin\\nfrom sklearn.experimental import enable_iterative_imputer  # This enables IterativeImputer\\nfrom sklearn.impute import IterativeImputer\\nfrom sklearn.linear_model import BayesianRidge\\n\\n# Transformer: Replace 0 with random longitude\\nclass LongitudeRandomImputer(BaseEstimator, TransformerMixin):\\n    def __init__(self, column=\\'longitude\\', min_val=29.2, max_val=40.4):\\n        self.column = column\\n        self.min_val = min_val\\n        self.max_val = max_val\\n\\n    def fit(self, X, y=None):\\n        return self\\n\\n    def transform(self, X):\\n        X = X.copy()\\n        X[self.column] = X[self.column].replace(0, np.nan)\\n        X[self.column] = X[self.column].apply(\\n            lambda x: np.random.uniform(self.min_val, self.max_val) if pd.isna(x) else x\\n        )\\n        return X\\n\\n# Transformer: Group-based mode imputation with global fallback\\nclass GroupModeImputer(BaseEstimator, TransformerMixin):\\n    def __init__(self, col, group_cols):\\n        self.col = col\\n        self.group_cols = group_cols\\n        self.global_mode = None\\n\\n    def fit(self, X, y=None):\\n        self.global_mode = X[self.col].mode(dropna=True)[0]\\n        return self\\n\\n    def transform(self, X):\\n        X = X.copy()\\n        try:\\n            X[self.col] = X.groupby(self.group_cols)[self.col].transform(\\n                lambda x: x.fillna(x.mode()[0]) if not x.mode().empty else x\\n            )\\n        except Exception as e:\\n            print(f\"Warning during group imputation of {self.col}: {e}\")\\n        X[self.col].fillna(self.global_mode, inplace=True)\\n        return X\\n\\n# Transformer: Iterative imputer for numeric columns\\nclass IterativeNumericImputer(BaseEstimator, TransformerMixin):\\n    def __init__(self, columns):\\n        self.columns = columns\\n        self.imputer = IterativeImputer(estimator=BayesianRidge(), random_state=42, max_iter=10)\\n\\n    def fit(self, X, y=None):\\n        self.imputer.fit(X[self.columns])\\n        return self\\n\\n    def transform(self, X):\\n        X = X.copy()\\n        X[self.columns] = self.imputer.transform(X[self.columns])\\n        return X\\n'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.experimental import enable_iterative_imputer  # This enables IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "# Transformer: Replace 0 with random longitude\n",
    "class LongitudeRandomImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column='longitude', min_val=29.2, max_val=40.4):\n",
    "        self.column = column\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[self.column] = X[self.column].replace(0, np.nan)\n",
    "        X[self.column] = X[self.column].apply(\n",
    "            lambda x: np.random.uniform(self.min_val, self.max_val) if pd.isna(x) else x\n",
    "        )\n",
    "        return X\n",
    "\n",
    "# Transformer: Group-based mode imputation with global fallback\n",
    "class GroupModeImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, col, group_cols):\n",
    "        self.col = col\n",
    "        self.group_cols = group_cols\n",
    "        self.global_mode = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.global_mode = X[self.col].mode(dropna=True)[0]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        try:\n",
    "            X[self.col] = X.groupby(self.group_cols)[self.col].transform(\n",
    "                lambda x: x.fillna(x.mode()[0]) if not x.mode().empty else x\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Warning during group imputation of {self.col}: {e}\")\n",
    "        X[self.col].fillna(self.global_mode, inplace=True)\n",
    "        return X\n",
    "\n",
    "# Transformer: Iterative imputer for numeric columns\n",
    "class IterativeNumericImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.imputer = IterativeImputer(estimator=BayesianRidge(), random_state=42, max_iter=10)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.imputer.fit(X[self.columns])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[self.columns] = self.imputer.transform(X[self.columns])\n",
    "        return X\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bc84ba90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.pipeline import Pipeline\\n\\n# List of all imputers and their logic\\nimputation_pipeline = Pipeline(steps=[\\n    ('longitude_random', LongitudeRandomImputer()),\\n\\n    ('funder_impute', GroupModeImputer('funder', ['region', 'lga', 'ward'])),\\n    ('installer_impute', GroupModeImputer('installer', ['region', 'funder'])),\\n    ('subvillage_impute', GroupModeImputer('subvillage', ['lga', 'ward'])),\\n    ('public_meeting_impute', GroupModeImputer('public_meeting', ['region', 'ward'])),\\n    ('scheme_name_impute', GroupModeImputer('scheme_name', ['basin', 'region', 'longitude', 'latitude'])),\\n    ('scheme_management_impute', GroupModeImputer('scheme_management', ['scheme_name', 'source', 'region'])),\\n    ('permit_impute', GroupModeImputer('permit', ['region', 'source'])),\\n\\n    ('iterative_numeric', IterativeNumericImputer(['construction_year', 'population', 'latitude', 'longitude', 'gps_height']))\\n])\\n\""
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# List of all imputers and their logic\n",
    "imputation_pipeline = Pipeline(steps=[\n",
    "    ('longitude_random', LongitudeRandomImputer()),\n",
    "\n",
    "    ('funder_impute', GroupModeImputer('funder', ['region', 'lga', 'ward'])),\n",
    "    ('installer_impute', GroupModeImputer('installer', ['region', 'funder'])),\n",
    "    ('subvillage_impute', GroupModeImputer('subvillage', ['lga', 'ward'])),\n",
    "    ('public_meeting_impute', GroupModeImputer('public_meeting', ['region', 'ward'])),\n",
    "    ('scheme_name_impute', GroupModeImputer('scheme_name', ['basin', 'region', 'longitude', 'latitude'])),\n",
    "    ('scheme_management_impute', GroupModeImputer('scheme_management', ['scheme_name', 'source', 'region'])),\n",
    "    ('permit_impute', GroupModeImputer('permit', ['region', 'source'])),\n",
    "\n",
    "    ('iterative_numeric', IterativeNumericImputer(['construction_year', 'population', 'latitude', 'longitude', 'gps_height']))\n",
    "])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "761cf2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Separate target from predictors\\ny = pump_data_clean.status_group\\nX = pump_data_clean.drop(['status_group'], axis=1)\\n\\n# Divide data into training and validation subsets\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_valid, y_train, y_valid= train_test_split(X, y, train_size=0.8, test_size=0.2,\\n                                                                random_state=0)\\nX_train = X_train.copy()\\nX_valid = X_valid.copy() \\n\""
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Separate target from predictors\n",
    "y = pump_data_clean.status_group\n",
    "X = pump_data_clean.drop(['status_group'], axis=1)\n",
    "\n",
    "# Divide data into training and validation subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid= train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "X_train = X_train.copy()\n",
    "X_valid = X_valid.copy() \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ed8614b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Fit and transform the training data\\nX_train_full = imputation_pipeline.fit_transform(X_train_full)\\n\\n# Only transform the validation set\\nX_valid_full= imputation_pipeline.transform(X_valid_full)\\n'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Fit and transform the training data\n",
    "X_train_full = imputation_pipeline.fit_transform(X_train_full)\n",
    "\n",
    "# Only transform the validation set\n",
    "X_valid_full= imputation_pipeline.transform(X_valid_full)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "343f000d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Drop columns\\ncols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()] \\nX_train_full.drop(cols_with_missing, axis=1, inplace=True)\\nX_valid_full.drop(cols_with_missing, axis=1, inplace=True)\\n\\n# \"Cardinality\" means the number of unique values in a column\\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \\n                        X_train_full[cname].dtype == \"object\"]\\n\\n# Select numerical columns\\nnumerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in [\\'int64\\', \\'float64\\']]\\n\\n# Keep selected columns only\\nmy_cols = low_cardinality_cols + numerical_cols\\nX_train = X_train[my_cols].copy()\\nX_valid = X_valid[my_cols].copy()'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "# Drop columns\n",
    "cols_with_missing = [col for col in X_train_full.columns if X_train_full[col].isnull().any()] \n",
    "X_train_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "X_valid_full.drop(cols_with_missing, axis=1, inplace=True)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "low_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = low_cardinality_cols + numerical_cols\n",
    "X_train = X_train[my_cols].copy()\n",
    "X_valid = X_valid[my_cols].copy()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8372c503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Get list of categorical variables\\ns = (X_train.dtypes == \\'object\\')\\nobject_cols = list(s[s].index)\\n\\nprint(\"Categorical variables:\")\\nprint(object_cols)\\n'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Get list of categorical variables\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe66ca22",
   "metadata": {},
   "source": [
    "## Define Quality for each approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "32fc3eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.metrics import accuracy_score\\n\\ndef score_dataset(X_train, X_valid, y_train, y_valid):\\n    model = RandomForestClassifier(n_estimators=100, random_state=0)\\n    model.fit(X_train, y_train)\\n    preds = model.predict(X_valid)\\n    return accuracy_score(y_valid, preds)\\n'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8844576d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndrop_X_train = X_train.select_dtypes(exclude=[\\'object\\'])\\ndrop_X_valid = X_valid.select_dtypes(exclude=[\\'object\\'])\\n\\nprint(\"Accuracy from Approach 1 (Drop categorical variables):\")\\nprint(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))\\n'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "print(\"Accuracy from Approach 1 (Drop categorical variables):\")\n",
    "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9e3e8e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.preprocessing import OrdinalEncoder\\n\\n# Make copy to avoid changing original data \\nlabel_X_train = X_train.copy()\\nlabel_X_valid = X_valid.copy()\\n\\n# Apply ordinal encoder to each column with categorical data\\nordinal_encoder = OrdinalEncoder()\\nlabel_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\\nlabel_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\\n\\nprint(\"MAE from Approach 2 (Ordinal Encoding):\") \\nprint(score_dataset(label_X_train, label_X_valid, y_train, y_valid))\\n'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Make copy to avoid changing original data \n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Apply ordinal encoder to each column with categorical data\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
    "label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\n",
    "\n",
    "print(\"MAE from Approach 2 (Ordinal Encoding):\") \n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1ce63f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.preprocessing import OneHotEncoder\\n\\n# Apply one-hot encoder to each column with categorical data\\nOH_encoder = OneHotEncoder(handle_unknown=\\'ignore\\', sparse=False)\\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\\n\\n# One-hot encoding removed index; put it back\\nOH_cols_train.index = X_train.index\\nOH_cols_valid.index = X_valid.index\\n\\n# Remove categorical columns (will replace with one-hot encoding)\\nnum_X_train = X_train.drop(object_cols, axis=1)\\nnum_X_valid = X_valid.drop(object_cols, axis=1)\\n\\n# Add one-hot encoded columns to numerical features\\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\\n\\n# Ensure all columns have string type\\nOH_X_train.columns = OH_X_train.columns.astype(str)\\nOH_X_valid.columns = OH_X_valid.columns.astype(str)\\n\\nprint(\"MAE from Approach 3 (One-Hot Encoding):\") \\nprint(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))\\n'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "# Ensure all columns have string type\n",
    "OH_X_train.columns = OH_X_train.columns.astype(str)\n",
    "OH_X_valid.columns = OH_X_valid.columns.astype(str)\n",
    "\n",
    "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
